{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alcholic Anonymous News articles extraction from The Economic Times. Data Extraction of following parameters\n",
    "- Headline\n",
    "- Description\n",
    "- Author\n",
    "- Published Date\n",
    "- Category\n",
    "- Publication\n",
    "- News\n",
    "- URL\n",
    "- Keywords\n",
    "- Summary\n",
    "\n",
    "### Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options # enables options in web browser\n",
    "from selenium import webdriver # web-based automation tool for Python\n",
    "from newspaper import Article # Article scraping & curation\n",
    "from bs4 import BeautifulSoup # Python library for pulling data out of HTML and XML files\n",
    "from requests import get # standard for making HTTP requests in Python\n",
    "import pandas as pd # library written for data manipulation and analysis\n",
    "import sys, time #  System-specific parameters and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Empty lists for Alcholic Anonymous News Articles parameters data to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines, descriptions, dates, authors, news, keywords, summaries, urls, category, publication = [], [], [], [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the total no.of.pages by total no.of articles from google search results¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Alcoholics Anonymous site:economictimes.indiatimes.com'\n",
    "\n",
    "url = 'https://www.google.com/search?q=' + '+'.join(keyword.split())\n",
    "\n",
    "soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "try:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 680 results (0.23 seconds)\n",
    "    max_pages = round([int(s) for s in soup.select_one('div#resultStats').text.split() if s.isdigit()][0]/10)\n",
    "    max_pages = max_pages + 1\n",
    "except:\n",
    "    # Extracts the digits if it the resulted number without comma ','. eg: About 1,080 results (0.23 seconds)\n",
    "    max_pages = round(int(''.join(i for i in soup.select_one('div#resultStats').text if i.isdigit()))/10)\n",
    "    max_pages = max_pages + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates max_pages value through while loop. Scraping the Articles urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 : 8\r"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome(options=options)\n",
    "browser.get(url)\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        index +=1\n",
    "        page = browser.page_source\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        linky = [soup.select('.r')[i].a['href'] for i in range(len(soup.select('.r')))]\n",
    "        urls.extend(linky)\n",
    "        if index == max_pages:\n",
    "            break\n",
    "        browser.find_element_by_xpath('//*[@id=\"pnnext\"]/span[2]').click()\n",
    "        time.sleep(2)\n",
    "        sys.stdout.write('\\r' + str(index) + ' : ' + str(max_pages) + '\\r')\n",
    "        sys.stdout.flush()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove duplicates urls entries in the list by executing below line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Extracted URL's are : 52 <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "print(\"Total Extracted URL's are\" + ' : ' + str(len(urls)), type(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates urls through for loop. Scraping the Articles with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 55s.economictimes.indiatimes.com/news/hospitals/malaysia-ihh-healthcare-to-add-another-3000-beds-by-2017/477039550414320k/articleshow/10708108.cmsshow/45284007.cms7.cms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        # Parse the url to NewsPlease \n",
    "        soup = BeautifulSoup(get(url).text, 'lxml')\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        \n",
    "        # Extracts the Headlines\n",
    "        try:\n",
    "            try:\n",
    "                headlines.append(article.title.strip())\n",
    "            except:\n",
    "                headlines.append(soup.select_one('meta[property=\"og:title\"]')['content'].strip())\n",
    "        except:\n",
    "            headlines.append(None)\n",
    "            \n",
    "        # Extracts the Descriptions    \n",
    "        try:\n",
    "            try:\n",
    "                descriptions.append(soup.select_one('meta[property=\"og:description\"]')['content'].strip().replace('\\n', ' '))\n",
    "            except:\n",
    "                descriptions.append(article.meta_description.strip())\n",
    "        except:\n",
    "            descriptions.append(None)\n",
    "            \n",
    "        # Extracts the Authors\n",
    "        try:\n",
    "            try:\n",
    "                authors.append(json.loads(soup.select('script[type=\"application/ld+json\"]')[1].text)['author'][0]['name'])\n",
    "            except:\n",
    "                authors.append(article.authors.strip())\n",
    "        except:\n",
    "            authors.append(None)\n",
    "        \n",
    "        # Extracts the published dates\n",
    "        try:\n",
    "            try:\n",
    "                dates.append(json.loads(soup.select('script[type=\"application/ld+json\"]')[1].text)['datePublished'])\n",
    "            except:\n",
    "                dates.append(str(article.publish_date))\n",
    "        except:\n",
    "            dates.append(None)\n",
    "            \n",
    "        # Extracts the news articles\n",
    "        try:\n",
    "            try:\n",
    "                news.append(' '.join(json.loads(soup.select('script[type=\"application/ld+json\"]')[1].text)['articleBody'].split()))\n",
    "            except:\n",
    "                news.append(' '.join(article.text.split()).replace(\"\\'\\'\",\" \").replace(\"\\'\", \"\").replace(\" / \", \"\"))\n",
    "        except:\n",
    "            news.append(None)\n",
    "            \n",
    "        # Extracts the news category\n",
    "        try:\n",
    "            category.append(article.meta_data['category'])\n",
    "        except:\n",
    "            category.append(None)\n",
    "            \n",
    "        # Extracts the news publication\n",
    "        try:\n",
    "            publication.append(json.loads(soup.select('script[type=\"application/ld+json\"]')[1].text)['publisher']['name'])\n",
    "        except:\n",
    "            publication.append(None)\n",
    "\n",
    "        # Extracts Keywords and Summaries\n",
    "        try:\n",
    "            keywords.append(article.keywords)\n",
    "            summaries.append(article.summary)\n",
    "        except:\n",
    "            keywords.append(None)\n",
    "            summaries.append(None)\n",
    "                        \n",
    "    except:\n",
    "        headlines.append(None)\n",
    "        descriptions.append(None)\n",
    "        authors.append(None)\n",
    "        dates.append(None)\n",
    "        category.append(None)\n",
    "        publication.append(None)\n",
    "        news.append(None)\n",
    "        keywords.append(None)\n",
    "        summaries.append(None)\n",
    "\n",
    "    sys.stdout.write('\\r' + str(index) + ' : ' + str(url) + '\\r')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Array Length of each list to create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52 52 52 52 52 52 52 52 52\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines), len(descriptions), len(authors), len(dates), len(category), len(publication), len(news), len(keywords), len(summaries), len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a csv file after checking array length and droping the missing values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Descriptions</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Dates</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Articles</th>\n",
       "      <th>category</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>Source_URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alcoholics Anonymous News and Updates from The...</td>\n",
       "      <td>Alcoholics Anonymous News and Updates from The...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Why Make in India when you Fake in India 4 May...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[liquor, 2016, anonymous, alcoholics, rum, tim...</td>\n",
       "      <td>Why 'Make in India' when you 'Fake in India' 4...</td>\n",
       "      <td>https://economictimes.indiatimes.com/topic/Alc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Inner Power</td>\n",
       "      <td>The agnostics say they cant suddenly start bel...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-06-17 05:50:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>In a head-to-head evaluation of the Alcoholics...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[higher, inner, believe, alcoholics, step, say...</td>\n",
       "      <td>In a head-to-head evaluation of the Alcoholics...</td>\n",
       "      <td>https://economictimes.indiatimes.com/opinion/v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Correction begins with admission</td>\n",
       "      <td>Tiger Woods has now ‘rediscovered his childhoo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-02-24 05:00:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Tiger Woods has now ‘rediscovered’ his childho...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[union, apology, public, tiger, religion, corr...</td>\n",
       "      <td>Tiger Woods has now ‘rediscovered’ his childho...</td>\n",
       "      <td>https://economictimes.indiatimes.com/opinion/v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A simple case of sorry and thanks</td>\n",
       "      <td>Alcoholics Anonymous, the community that helps...</td>\n",
       "      <td>None</td>\n",
       "      <td>2006-11-07 00:10:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Alcoholics Anonymous, the community that helps...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[simple, person, randomly, alcoholics, case, s...</td>\n",
       "      <td>Which is why a typical individual course for a...</td>\n",
       "      <td>https://economictimes.indiatimes.com/articlesh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Go cold turkey, for god’s sake!</td>\n",
       "      <td>A friend who attended a workshop of Overeaters...</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-09-30 07:50:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>A friend who attended a workshop of Overeaters...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[governments, fact, cold, turkey, gods, religi...</td>\n",
       "      <td>You could go “work cold turkey” , and detoxify...</td>\n",
       "      <td>https://economictimes.indiatimes.com/opinion/e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Headlines  \\\n",
       "0  Alcoholics Anonymous News and Updates from The...   \n",
       "1                                        Inner Power   \n",
       "2                   Correction begins with admission   \n",
       "3                  A simple case of sorry and thanks   \n",
       "4                    Go cold turkey, for god’s sake!   \n",
       "\n",
       "                                        Descriptions Authors  \\\n",
       "0  Alcoholics Anonymous News and Updates from The...    None   \n",
       "1  The agnostics say they cant suddenly start bel...    None   \n",
       "2  Tiger Woods has now ‘rediscovered his childhoo...    None   \n",
       "3  Alcoholics Anonymous, the community that helps...    None   \n",
       "4  A friend who attended a workshop of Overeaters...    None   \n",
       "\n",
       "             Published_Dates Publication  \\\n",
       "0                       None        None   \n",
       "1  2014-06-17 05:50:00+00:00        None   \n",
       "2  2010-02-24 05:00:00+00:00        None   \n",
       "3  2006-11-07 00:10:00+00:00        None   \n",
       "4  2010-09-30 07:50:00+00:00        None   \n",
       "\n",
       "                                            Articles category  \\\n",
       "0  Why Make in India when you Fake in India 4 May...       {}   \n",
       "1  In a head-to-head evaluation of the Alcoholics...       {}   \n",
       "2  Tiger Woods has now ‘rediscovered’ his childho...       {}   \n",
       "3  Alcoholics Anonymous, the community that helps...       {}   \n",
       "4  A friend who attended a workshop of Overeaters...       {}   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [liquor, 2016, anonymous, alcoholics, rum, tim...   \n",
       "1  [higher, inner, believe, alcoholics, step, say...   \n",
       "2  [union, apology, public, tiger, religion, corr...   \n",
       "3  [simple, person, randomly, alcoholics, case, s...   \n",
       "4  [governments, fact, cold, turkey, gods, religi...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  Why 'Make in India' when you 'Fake in India' 4...   \n",
       "1  In a head-to-head evaluation of the Alcoholics...   \n",
       "2  Tiger Woods has now ‘rediscovered’ his childho...   \n",
       "3  Which is why a typical individual course for a...   \n",
       "4  You could go “work cold turkey” , and detoxify...   \n",
       "\n",
       "                                         Source_URLs  \n",
       "0  https://economictimes.indiatimes.com/topic/Alc...  \n",
       "1  https://economictimes.indiatimes.com/opinion/v...  \n",
       "2  https://economictimes.indiatimes.com/opinion/v...  \n",
       "3  https://economictimes.indiatimes.com/articlesh...  \n",
       "4  https://economictimes.indiatimes.com/opinion/e...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(headlines) == len(descriptions) == len(authors) == len(dates) == len(news) == len(publication) == len(keywords) == len(summaries) == len(urls) == len(category):\n",
    "    tbl = pd.DataFrame({'Headlines' : headlines,\n",
    "                        'Descriptions' : descriptions,\n",
    "                        'Authors' : authors,\n",
    "                        'Published_Dates' : dates,\n",
    "                        'Publication' : publication,\n",
    "                        'Articles' : news,\n",
    "                        'category' : category,\n",
    "                        'Keywords' : keywords,\n",
    "                        'Summaries' : summaries,\n",
    "                        'Source_URLs' : urls})\n",
    "    tbl.dropna()\n",
    "    path = 'C:\\\\Users\\\\GM\\\\Desktop\\\\!Code!\\\\CDRI\\\\Alcoholics Anonymous\\\\Data Extraction\\\\#Dataset\\\\'\n",
    "    tbl.to_csv(path+'The_Economic_Times.csv', index=False)\n",
    "else:\n",
    "    print('Array lenght does not match!')\n",
    "\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
